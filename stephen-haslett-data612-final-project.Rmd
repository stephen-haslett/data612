---
title: "DATA 612 Final Project"
author: "Stephen Haslett"
date: "07/10/2020"
output:
  html_document:
    theme: cosmo

subtitle: 'Amazon Magazine Subscriptions Recommender System'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(sparklyr)
library(dplyr)
library(plyr)
library(RCurl)
library(jsonlite)
library(knitr)
library(kableExtra)
library(ggplot2)
```


## Assignment Instructions
Build out the system that you described in your final project planning document.

## Introduction
My original objective for this project was to create a recommendation system that predicted Amazon beauty product ratings. However, the compact version of the [*Amazon - Ratings (Beauty Products)*](https://www.kaggle.com/skillsmuggler/amazon-ratings) dataset (*Available from [Kaggle](https://www.kaggle.com)*) was missing metadata, and thus it did not contain the names of the beauty products.

I attempted to get around this issue by using the full version of the dataset (*Available from [UCSD](http://deepyeti.ucsd.edu/jianmo/amazon/index.html)*) that contained the metadata that I needed, but the files were too large to run locally. This forced me to go with a smaller dataset (Amazon Magazine Subscription Ratings) that was small enough to run locally, and also contained the metadata that I needed.

## Project Objective
The objective of this project is to build a recommendation system that predicts magazine subscription ratings. The dataset for this project is considerably larger than those of previous projects, so I will leverage Spark to store the data. In order to integrate Spark with R, I will utilize the R *sparklyr* package. For the recommendation model, I will use the
Alternating Least Square (ALS) matrix factorization algorithm.

## Data Source
As noted in the introduction section above, I will use the Amazon Magazine Subscription Ratings dataset made available by the University of California San Diego. The dataset was compiled in 2018, and is split into 2 json files - a reviews file, and a metadata file. The reviews file contains 89,689 magazine subscription reviews, and the metadata file contains 3,493 products.


## Data Manipulation

### Pull in the datasets from GitHub
In order to make the json data files accessable outside of the confines of my local machine, I will store them in Github, and then ingest them using the jsonlite package.
```{r, message=FALSE, warning=FALSE}
# Import the Json data from GitHub.
amazon_meta_source <- url('https://raw.githubusercontent.com/stephen-haslett/data612/612-final-project/amazon_magazine_meta.json')
amazon_ratings_source <- url('https://raw.githubusercontent.com/stephen-haslett/data612/612-final-project/amazon_magazine_ratings.json')

amazon_magazine_metadata <- do.call(rbind, lapply(paste(readLines(amazon_meta_source, warn = FALSE), collapse = ''), jsonlite::fromJSON))
amazon_magazine_ratings <- do.call(rbind, lapply(paste(readLines(amazon_ratings_source, warn = FALSE), collapse = ''), jsonlite::fromJSON))
```

### Explore the dataset column names.
Metadata column names.
```{r, message=FALSE, warning=FALSE}
knitr::kable(names(amazon_magazine_metadata), format = 'html') %>%
  kableExtra::kable_styling(bootstrap_options = c('striped', 'hover')) %>% 
  add_header_above(c('Metadata Column Names' = 1))
```

Ratings column names.
```{r, message=FALSE, warning=FALSE}
knitr::kable(names(amazon_magazine_ratings), format = 'html') %>%
  kableExtra::kable_styling(bootstrap_options = c('striped', 'hover')) %>% 
  add_header_above(c('Ratings Column Names' = 1))
```


### Remove the columns that we don't need.
There are quite a few columns in the datasets that we do not need. The only columns that we are interested in from the metadata set are
the 'asin' (product ID), and the 'title' (product name) columns, so we can remove all the other columns. The same goes for the ratings set;
we only need the asin, reviewerID, reviewerName, and overall (rating) columns.
```{r, message=FALSE, warning=FALSE}
# Select the columns we need from the metadata dataset.
metadata_refined <- select(amazon_magazine_metadata, asin, title)
# We only want items that have product titles, so remove all the rows that do not have a title.
metadata_refined <- metadata_refined[!is.na(metadata_refined$title),]

# Select the columns we need from the metadata dataset.
ratings_refined <- select(amazon_magazine_ratings, asin, reviewerID, reviewerName, overall)
```


### Combine the metadata and ratings datasets.
Now that we have the columns that we need from both datasets, we can combine the dataset using a join. Both sets share the asin column,
so we can perform the join using the 'asin' column. This will leave us with a table that contains both the user review data, and the item ids
and product names. 
```{r, message=FALSE, warning=FALSE}
magazine_ratings <- join(ratings_refined, metadata_refined, by = 'asin', type = 'right')

# Remove rows that are missing reviewerID values.
magazine_ratings <- magazine_ratings[!is.na(magazine_ratings$reviewerID),]

# Rename column names to make them more descriptive.
names(magazine_ratings)[1] <- 'item_id'
names(magazine_ratings)[2] <- 'user_id'
names(magazine_ratings)[3] <- 'user_name'
names(magazine_ratings)[4] <- 'rating'
names(magazine_ratings)[5] <- 'item_name'
```

### Convert the item IDs and user IDs to numeric values.
The item_ids and user_ids are alphanumeric. When we create the model, we use sparklyr's "ml_als()" function. The
function's 'user_col' and 'item_col' parameter values must be integers, so we need to convert these to
integer values. To achieve this, we will first convert these values to factors, so that we can then convert them to integers. 
```{r, message=FALSE, warning=FALSE}
item_factor <-as.factor(magazine_ratings$item_id)
item_id <- as.integer(item_factor)
magazine_ratings$item_id <- item_id

user_factor <- as.factor(magazine_ratings$user_id)
user_id <- as.integer(user_factor)
magazine_ratings$user_id <- user_id

knitr::kable(head(magazine_ratings, 10), format = 'html') %>%
  kableExtra::kable_styling(bootstrap_options = c('striped', 'hover')) %>% 
  add_header_above(c('Refined Ratings Dataset with Numeric Item and User IDs' = 5))
```


## Distribution of Ratings
With the alterations to our dataset complete, we are now in a position to take a look at the ratings distribution.
As we can see from the below bar plot, the majority of magazine subscriptions were given high ratings, with fewer items being
given low ratings. 
```{r, message=FALSE, warning=FALSE}
magazine_ratings %>% 
  ggplot(aes(rating)) +
  geom_bar(fill = 'darkgreen') +
  labs(title = 'Distribution of Ratings', y = 'Frequency', x = 'Ratings') +
  theme_minimal()
```

## Copy the data over to Spark.
Now that we have a table that contains only the values that we need, we can copy the data over to Spark.
Because The dataset was too large for my local machine to handle, I needed to reduce the number of records to send over to Spark.
To accomplish this, I selected the first 2000 records.
```{r, message=FALSE, warning=FALSE}
# Select the first 2000 records from the dataset to accommodate for limited resources on my local machine.
magazine_ratings <- head(magazine_ratings, 2000)

# Connect to Spark and copy over the dataset.
sc <- spark_connect(master = 'local', version = '3.0.0')
ratings <- sdf_copy_to(sc, magazine_ratings, overwrite = TRUE)
```


## Split the data into test and training sets.
Now that the data is in Spark, we can split the dataset into test and training sets using a ratio of 70:25
(70% of the entire dataset for training, and 25% for testing). 
```{r, message=FALSE, warning=FALSE}
partition <- ratings %>%  sdf_partition(training = 0.75, test = 0.25, seed = 1099)
training <- partition$training
test <- partition$test
```


## Create the Alternating Least Squares (ALS) model.
To create the ALS model, I used the Sparklyr's ml_als() function. After running my predictions, I found that a lot of predictions had NaN values, which is less than ideal.
Thankfully, the ml_als() function provides a useful parameter that takes care of this issue - "cold_start_strategy". Setting this parameter to "drop" removes rows containing
Nan prediction values from the dataframe.   
```{r, message=FALSE, warning=FALSE}
als_model <- ml_als(training, rating_col = 'rating', user_col = 'user_id', item_col = 'item_id', cold_start_strategy = 'drop')

knitr::kable(summary(als_model), format = 'html') %>%
  kableExtra::kable_styling(bootstrap_options = c('striped', 'hover')) %>% 
  add_header_above(c('ALS Model Summary' = 4))
```

## Perform predictions on the data.
```{r, message=FALSE, warning=FALSE}
predictions <- als_model$.jobj %>%
  invoke('transform', spark_dataframe(test)) %>%
  collect()

knitr::kable(predictions, format = 'html') %>%
  kableExtra::kable_styling(bootstrap_options = c('striped', 'hover'), fixed_thead = T) %>% 
  add_header_above(c('Prediction Results' = 6))
```


## Conclusion
