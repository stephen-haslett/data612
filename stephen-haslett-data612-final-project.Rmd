---
title: "DATA 612 Final Project"
author: "Stephen Haslett"
date: "07/10/2020"
output:
  html_document:
    theme: cosmo

subtitle: 'Amazon Magazine Subscriptions Recommender System'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(sparklyr)
library(dplyr)
library(plyr)
library(RCurl)
library(jsonlite)
```


## Assignment Instructions
Build out the system that you described in your final project planning document.

## Data Manipulation

### Pull in the datasets from GitHub
```{r, message=FALSE, warning=FALSE}
# Import the Json data from GitHub.
amazon_meta_source <- url('https://raw.githubusercontent.com/stephen-haslett/data612/612-final-project/amazon_magazine_meta.json')
amazon_ratings_source <- url('https://raw.githubusercontent.com/stephen-haslett/data612/612-final-project/amazon_magazine_ratings.json')

amazon_magazine_metadata <- do.call(rbind, lapply(paste(readLines(amazon_meta_source, warn = FALSE), collapse = ''), jsonlite::fromJSON))
amazon_magazine_ratings <- do.call(rbind, lapply(paste(readLines(amazon_ratings_source, warn = FALSE), collapse = ''), jsonlite::fromJSON))

head(amazon_magazine_metadata, 100)
head(amazon_magazine_ratings, 100)
```

### Remove the columns that we don't need.
```{r, message=FALSE, warning=FALSE}

metadata_refined <- select(amazon_magazine_metadata, asin, title)
# We only want items that have titles.
metadata_refined <- metadata_refined[!is.na(metadata_refined$title),]

ratings_refined <- select(amazon_magazine_ratings, asin, reviewerID, reviewerName, overall)
```


### Join the 2 tables together.
```{r, message=FALSE, warning=FALSE}
magazine_ratings <- join(ratings_refined, metadata_refined, by = 'asin', type = 'right')

# Rename the 'overall' column name to 'rating'.
names(magazine_ratings)[4] <- "rating"

head(magazine_ratings)
```


## Copy the data over to Spark.
The dataset is too large for my local machine to handle, so I had to reduce the number of records to send over to Spark.
```{r, message=FALSE, warning=FALSE}
# Select the first 2000 records from the dataset to accommodate for limited resources on my local machine. 
magazine_ratings <- head(magazine_ratings, 2000)
# Connect to Spark and copy over the dataset.
sc <- spark_connect(master = 'local', version = '3.0.0')
ratings <- sdf_copy_to(sc, magazine_ratings, overwrite = TRUE)


# Split the data into test and training sets.
split_data <- ratings %>% sdf_random_split(training = 0.75, testing = 0.25)
```




