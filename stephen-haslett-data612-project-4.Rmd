---
title: "Data 612 Project 4 | Accuracy and Beyond"
author: "Stephen Haslett"
date: "6/28/2020"
output:
  rmdformats::readthedown
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(kableExtra)
library(recommenderlab)
library(ggplot2)
library(rmdformats)
library(rsvd)
library(BBmisc)
```

## Assignment Instructions
1. As in your previous assignments, compare the accuracy of at least two recommender system algorithms against your offline data.

2. Implement support for at least one business or user experience goal such as increased serendipity, novelty, or diversity.

3. Compare and report on any change in accuracy before and after youâ€™ve made the change in #2.

4. As part of your textual conclusion, discuss one or more additional experiments that could be performed and/or metrics that could be
   evaluated only if online evaluation was possible. Also, briefly propose how you would design a reasonable online evaluation environment.

## Introduction
For my previous projects, I used the MovieLense dataset so for this project, I will be using the Jester joke rating dataset - http://eigentaste.berkeley.edu/dataset. The recommederlab package ships with a thinned down version of the orignal version, so I will be using this version of the dataset. 

## Initial Data Analysis

### Load the Jester5k dataset
```{r, warning=FALSE, message=FALSE}
set.seed(150)
data(Jester5k)
show(Jester5k)
```


The following image displays the first 25 users' ratings of jokes 1 through 40.  
```{r, warning=FALSE, message=FALSE}
image(Jester5k[1:25, 1:40], "Jester Ratings Sample")
```

The red cells represent positively rated jokes, the blue cells represent negatively rated items, and the white cells represent jokes without ratings.

### Distribution of ratings

```{r, warning=FALSE, message=FALSE}
hist(getRatings(Jester5k),
     main = 'Distribution of Ratings',
     ylab = 'Frequency',
     xlab = 'Rating',
     col = 'Tomato')
```

The ratings distribution tells us that the majority of jokes in our subset were given positive ratings. However,
a comparatively small amount of jokes in the set earned the higest rating of 10.

### Average rating across all jokes.
```{r, warning=FALSE, message=FALSE}
mean_rating <- colMeans(Jester5k)
quantile(mean_rating)
```


```{r, warning=FALSE, message=FALSE}
qplot(mean_rating,
     main = 'Distribution of Average Ratings',
     xlab = 'Average Rating')
```

A look at the average ratings across all jokes shows us that the average rating for a good joke is 1.

### Recommender Algorithms

In this section, I am going to explore the performance of the UBCF, and IBCF recommender algorithms. Additionally, in order to satisfy
requirement 2 of the assignment (introducting serendipity and novelty), I will utilize the POPULAR and RANDOM algorithms.


```{r, warning=FALSE, message=FALSE}
set.seed(1)
# Define an evaluation scheme and pass it to all 5 models.
evaluation <- evaluationScheme(Jester5k, method = 'split', train = 0.8, k = 1, given = 10, goodRating = 1)
user_based_model <- Recommender(getData(evaluation, 'train'), 'UBCF')
item_based_model <- Recommender(getData(evaluation, 'train'), 'IBCF')
popular_model <- Recommender(getData(evaluation, 'train'), 'POPULAR')
random_model <- Recommender(getData(evaluation, 'train'), 'RANDOM')
```



#### Prediction Accuracy
```{r, warning=FALSE, message=FALSE}
user_predict <- predict(user_based_model, getData(evaluation, 'known'), type = 'ratings')
item_predict <- predict(item_based_model, getData(evaluation, 'known'), type = 'ratings')
popular_predict <- predict(popular_model, getData(evaluation, 'known'), type = 'ratings')
random_predict <- predict(random_model, getData(evaluation, 'known'), type = 'ratings')

results <- rbind(
  'User Model Accuracy' = calcPredictionAccuracy(user_predict, getData(evaluation, 'unknown')), 
  'Item Model Accuracy' = calcPredictionAccuracy(item_predict, getData(evaluation, 'unknown')),
  'Popular Model Accuracy' = calcPredictionAccuracy(popular_predict, getData(evaluation, 'unknown')),
  'Random Model Accuracy' = calcPredictionAccuracy(random_predict, getData(evaluation, 'unknown'))
)
knitr::kable(results, format = 'html') %>%
  kableExtra::kable_styling(bootstrap_options = c('striped', 'hover'))

```

```{r, warning=FALSE, message=FALSE}
models <- list('User Based Model' = list(name = 'UBCF', param = list(nn = 50)),
               'Item Based Model' = list(name = 'IBCF', param = list(k = 50)),
               'Popular Model' = list(name = 'POPULAR', param = NULL),
               'Random Model' = list(name = 'RANDOM', param = NULL))

results <- evaluate(evaluation, models, type = 'topNList', n = c(1, 5, 10, 20, 30, 50))
results
```

### Analysis

#### Precision Recall
```{r, warning=FALSE, message=FALSE}
plot(results, 'prec/rec', annotate = c(1, 3))
```



#### ROC
```{r, warning=FALSE, message=FALSE}
plot(results, 'ROC', annotate = c(4, 3))
```


